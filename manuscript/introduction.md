# Introduction

*This book contains recipes for wrangling FutureLearn course data using Python and R. It is presented as a work in progress, published via the Leanpub publishing platform.*

As with all learning environments operated by schools, colleges, universities and other learning providers, the FutureLearn platfrom collects data about a wide range of learner interactions with the platfrom. The burgeoning field of learning analytics aims to harness this data in order to provide a range of tools for monitoring and reporting on learning activity, often with the aim of predicting learning behaviour, identifying at risk learners, or personalising the the offering provided to invididual learners.

My own preference is to see the data collected as a reflection of the performance of the course materials themselves. That is, I see the data as being useful to *course authors* and *course designers* who want to better understand how learners interact with and pass through the materials *en masse* so that the performance of the materials themselves, rather than that of individal learners, can be assessed.

The origins of this book lay in the Open University / FutureLearn course [Learn to code for Data analysis](https://www.futurelearn.com/courses/learn-to-code), first presented in Autumn 2015. The course taught learners how to use the Python programming language, and more specifically the `pandas` data analysis Python package, to perform a range of data analysis and visualisation tasks. The data used in the course included freely available opendata from the World Bank and the United Nations. The programming environment used in the course was a browser based environment known as Jupyter notebooks. The notebook style programming interface differs from traditional programming environments (though it will be familiar to users of mathematical computing enviornments such as Mathematica), where code is written in often austere and intimidating text editor environment. Instead, the notebook environment blends editiable text areas with executable code cells whose output can be displayed inline in the notebook document once the code is run. The notebooks themselves are edited via a web browser, and served in the background either by a desktop application or an online service such as SageMathCloud.

As the course was being delivered, the course team was provided with occasional data downloads detailing some of the interactions occurring on the platform: enrolment and unenrolment figures, data about comment or quiz activity, and data describing when learners first accessed, or last marked as complete, each step of the course. It seemed to me in-keeping with a philosophy of my original faculty at the Open University, the Faculty of Technology, as was, that we should use the content and context of the course reflexively in the analysis of the data produced from the course. 

*I had also pushed for using the notebooks as the production environment of the course - the notebooks are authored in the markdown text format which is the format used to create FutureLearn courses. As it was, our course materials went from markdown, into Microsoft Word for editing, then back to markdown for entry into the FutureLearn platform. And then we had to work out how fix the errors. This book, in part, is an attempt to reclaim the use of the notebooks as an authoring environment. The Leanpub platform constructs books from one or more markdown manuscript files, which I have created using the notebooks and, more directly, by editing files direclty in Github, which acts the "source code repository" for the manuscript files used to create the book. The RStudio environment too can publish to markdown (I technique I used to create the [Wrangling F1 Data With R](https://leanpub.com/wranglingf1datawithr/) Leanpub book) and I fully expect to use that route when it comes to adding in some R recipes.*

And so to the book itself. The original Python/`pandas`recipes themselves started out life as recipes that used only the techniques that had been taught in the *Learn to Code for Data Analysis* course, although they were updated and slightly expanded for the "first commit" of the book. In particular, I added in some interactive features that allow interactive widgets to be embedded in the notebook to allow you to manipular the data directly through drop down selection lists and numerical sliders selectors.

The book as presented is a *static* beast, rendered in HTML and incapable of executing the code fragments. The book is also skeletal and unillustrated. Whilst FutureLearn data *can* be analysed and the results of the data *can* be shared publicly, the data is not openly licensed and permission is required before any analyses can first be shared. And I don't do permission. The ethics around what I consider fair game for analysis, and what I don't, are also my own.â’¶

But that is not to say the code cannot be run. The book is a rendering of hybrid text and code notebooks (original *.ipynb* files for the Jupyter notebooks, *Rmd* for content authored in RStudio), the original versions of which can be found in the Github repository where all the source materials for this book can be found: [psychemedia/futurelearnStatsSketches](https://github.com/psychemedia/futurelearnStatsSketches/).

There's one final innovation(?), invention(?), experiment(?) I'm pursuing here, and that focuses on sustainability. The majority of effort associated with producing this work was done in my own time, on the one day of the working week (as well as evenings and weekends) that my 0.8FTE Open University contract does not pay me for. It's my own learning time, my own play time, though it often feeds back into my OU activities, and whilst I'd originally hoped to spend the the time engaged in profitable consulting activities, it hasn't worked out like that. So here's the rub: this book hasn't been funded by a research project, indeed hasn't really been funded at all. It's not proper research and I make no real claims of it (it's the output of my playing with and learning how to use a range data analysis tools; it *is* my *learning diary* and a record of my learning journey). But it is something I'm happy to share, and something I may spend time on exploring further, as well as perhaps trekking around the country to talk about - if it can cover its own expenses.

So if the contents of this work save you time, make a donation. Leanpub supports a flexible pricing policy and doesn't take a huge cut (though the tax man does take his fair share). The price of this book starts at *free* and goes up from there. You can pay as much or as little as you want, and pay as many times as you want. Once you've got a copy of the book, you get all future updates to it *for free*, unless you want to pay again. If this was an article you wanted to read in an academic journal, and your library didn't have a copy, it would cost you what? Up to $50 dollars or so? If this was a training session provided from a technical trainer, how much would the quarter-day, half-day or full-day seat cost?

*But WTF?*, you may think, *isn't this moonlighting, isn't this double pay on top of your OU job?*. So here's the deal: any monies raised from the sale of this book will go solely on covering expenses that *I* deem relevant to exploring the ideas contained in this work (and that's the caveat: *I, me, my opinion*). This may include things like web hosting (so I can host online interactive versions of the code), or paying Github for a paid for plan, or traveling to relevant events. (Sustainability cuts several ways: I'm feeding Leanpub from Github, so if Github goes away, I have more work to do in finding an alternative publication route; such as using Dropbox; which also has a paid plan (although income from my F1 data book currently pays for that...) But so you can see any the money goes, I'll publish information about any and all receipts and expenses.

*Tony Hirst, April 2016*
